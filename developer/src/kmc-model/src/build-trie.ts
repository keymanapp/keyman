import { ModelCompilerError, ModelCompilerMessageContext, ModelCompilerMessages } from "./model-compiler-messages.js";
import { callbacks } from "./compiler-callbacks.js";

import { TrieBuilder } from '@keymanapp/models-templates';

// Supports LF or CRLF line terminators.
const NEWLINE_SEPARATOR = /\u000d?\u000a/;

/**
 * A word list is (conceptually) an array of pairs: the concrete word form itself + a
 * non-negative count.
 *
 * Since each word should only appear once within the list, we represent it with
 * an associative array pattern keyed by the wordform.
 */
export type WordList = {[wordform: string]: number};

/**
 * Returns a data structure that can be loaded by the TrieModel.
 *
 * It implements a **weighted** trie, whose indices (paths down the trie) are
 * generated by a search key, and not concrete wordforms themselves.
 *
 * @param sourceFiles an array of source files that will be read to generate the trie.
 */
export function createTrieDataStructure(filenames: string[], searchTermToKey?: (wf: string) => string): string {
  if (typeof searchTermToKey !== "function") {
    throw new ModelCompilerError(ModelCompilerMessages.Error_SearchTermToKeyMustBeExplicitlySpecified());
  }
  // Make one big word list out of all of the filenames provided.
  const wordlist: WordList = {};
  filenames.forEach(filename => parseWordListFromFilename(wordlist, filename));

  const trie = buildTrie(wordlist, searchTermToKey as SearchTermToKey);
  return JSON.stringify(trie);
}

/**
 * Parses a word list from a file, merging duplicate entries.
 *
 * The word list may be encoded in:
 *
 *  - UTF-8, with or without BOM [exported by most software]
 *  - UTF-16, little endian, with BOM [exported by Microsoft Excel]
 *
 * @param wordlist word list to merge entries into (may have existing entries)
 * @param filename filename of the word list
 */
export function parseWordListFromFilename(wordlist: WordList, filename: string): void {
  ModelCompilerMessageContext.filename = filename;
  _parseWordList(wordlist, new WordListFromFilename(filename));
}

/**
 * Parses a word list from a string. The string should have multiple lines
 * with LF or CRLF line terminators.
 *
 * @param wordlist word list to merge entries into (may have existing entries)
 * @param filename filename of the word list
 */
export function parseWordListFromContents(wordlist: WordList, contents: string): void {
  ModelCompilerMessageContext.filename = undefined;
  _parseWordList(wordlist, new WordListFromMemory(contents));
}

/**
 * Reads a tab-separated values file into a word list. This function converts all
 * entries into NFC and merges duplicate entries across wordlists. Duplication is
 * on the basis of character-for-character equality after normalisation to NFC.
 *
 * Format specification:
 *
 *  - the file is a UTF-8 encoded text file.
 *  - new lines are either LF or CRLF.
 *  - the file MAY start with the UTF-8 byte-order mark (BOM); that is, if the
 *    first three bytes of the file are EF BB BF, these will be interepreted as
 *    the BOM and will be ignored.
 *  - the file either consists of a comment or an entry.
 *  - comment lines MUST start with the '#' character on the very first column.
 *  - entries are one to three columns, separated by the (horizontal) tab
 *    character.
 *  - column 1 (REQUIRED): the wordform: can have any character except tab, CR,
 *    LF. Surrounding whitespace characters are trimmed.
 *  - column 2 (optional): the count: a non-negative integer specifying how many
 *    times this entry has appeared in the corpus. Blank means 'indeterminate';
 *    commas are permissible in the digits.
 *  - column 3 (optional): comment: an informative comment, ignored by the tool.
 *
 * @param wordlist word list to merge entries into (may have existing entries)
 * @param contents contents of the file to import
 */
function _parseWordList(wordlist: WordList, source:  WordListSource): void {
  const TAB = "\t";

  const wordsSeenInThisFile = new Set<string>();

  for (const [lineno, srcLine] of source.lines()) {
    ModelCompilerMessageContext.line = lineno;

    // Remove the byte-order mark (BOM) from the beginning of the string.
    // Because `contents` can be the concatenation of several files, we have to remove
    // the BOM from every possible start of file -- i.e., beginning of every line.
    const line = srcLine.replace(/^\uFEFF/, '').trim();

    if (line.startsWith('#') || line === "") {
      continue; // skip comments and empty lines
    }

    // The third column is the comment. Always ignored!
    let [wordform, countText] = line.split(TAB);

    // Clean the word form.
    const original = wordform;

    wordform = wordform.normalize('NFC');
    if (original !== wordform) {
      // Mixed normalization forms are yucky! Hint about it, because it may
      // result in unexpected counts where multiple normalization forms for one
      // word
      callbacks.reportMessage(ModelCompilerMessages.Hint_MixedNormalizationForms({wordform: wordform}));
    }

    wordform = wordform.trim()

    countText = (countText || '').trim().replace(/,/g, '');
    let count = parseInt(countText, 10);

    // When parsing a decimal integer fails (e.g., blank or something else):
    if (!isFinite(count) || count < 0) {
      // TODO: is this the right thing to do?
      // Treat it like a hapax legonmenom -- it exist, but only once.
      count = 1;
    }

    if (wordsSeenInThisFile.has(wordform)) {
      // The same word seen across multiple files is fine, but a word seen
      // multiple times in one file may be a problem
      callbacks.reportMessage(ModelCompilerMessages.Hint_DuplicateWordInSameFile({wordform: wordform}));
    }
    wordsSeenInThisFile.add(wordform);

    wordlist[wordform] = (isNaN(wordlist[wordform]) ? 0 : wordlist[wordform] || 0) + count;
  }
}

type LineNoAndText = [number, string];

interface WordListSource {
  readonly name: string;
  lines(): Iterable<LineNoAndText>;
}

class WordListFromMemory implements WordListSource {
  readonly name = '<memory>';
  private readonly _contents: string;

  constructor(contents: string) {
    this._contents = contents;
  }

  *lines() {
    yield *enumerateLines(this._contents.split(NEWLINE_SEPARATOR));
  }
}

class WordListFromFilename {
  readonly name: string;
  constructor(filename: string) {
    this.name = filename;
  }

  *lines() {
    const data = callbacks.loadFile(this.name);
    if(!data) {
      throw new ModelCompilerError(ModelCompilerMessages.Error_WordlistFileNotFound({filename:this.name}));
    }
    const contents = new TextDecoder(detectEncoding(data)).decode(data);
    yield *enumerateLines(contents.split(NEWLINE_SEPARATOR));
  }
}

/**
 * Yields pairs of [lineno, line], given an Array of lines.
 */
function* enumerateLines(lines: string[]): Generator<LineNoAndText> {
    let i = 1;
    for (const line of lines) {
      yield [i, line];
      i++;
    }
}

/**
 * An **opaque** type for a string that is exclusively used as a search key in
 * the trie. There should be a function that converts arbitrary strings
 * (queries) and converts them into a standard search key for a given language
 * model.
 *
 * Fun fact: This opaque type has ALREADY saved my bacon and found a bug!
 */
type SearchKey = string & { _: 'SearchKey'};

/**
 * A function that converts a string (word form or query) into a search key
 * (secretly, this is also a string).
 */
export interface SearchTermToKey {
  (wordform: string): SearchKey;
}

/**
 * Builds a trie from a word list.
 *
 * @param wordlist    The wordlist with non-negative weights.
 * @param keyFunction Function that converts word forms into indexed search keys
 * @returns A JSON-serialiable object that can be given to the TrieModel constructor.
 */
export function buildTrie(wordlist: WordList, keyFunction: SearchTermToKey): object {
  const collater = new TrieBuilder(keyFunction);

  buildFromWordList(collater, wordlist);
  return {
    totalWeight: collater.getTotalWeight(),
    root: collater.getRoot()
  }
}

/**
 * Populates the trie with the contents of an entire wordlist.
 * @param words a list of word and count pairs.
 */
function buildFromWordList(trieCollator: TrieBuilder, words: WordList): TrieBuilder {
  for (const [wordform, weight] of Object.entries(words)) {
    trieCollator.addEntry(wordform, weight);
  }
  trieCollator.sort();
  return trieCollator;
}

/**
 * Detects the encoding of a text file.
 *
 * Supported encodings are:
 *
 *  - UTF-8, with or without BOM
 *  - UTF-16, little endian, with BOM
 *
 * UTF-16 in big endian is explicitly NOT supported! The reason is two-fold:
 * 1) Node does not support it without resorting to an external library (or
 * swapping every byte in the file!); and 2) I'm not sure anything actually
 * outputs in this format anyway!
 *
 * @param filename filename of the file to detect encoding
 */
function detectEncoding(buffer: Uint8Array): 'utf-8' | 'utf-16le' {
  if(buffer.length < 2) {
    return 'utf-8';
  }

  // Note: BOM is U+FEFF
  // In little endian, this is 0xFF 0xFE
  if (buffer[0] == 0xFF && buffer[1] == 0xFE) {
    return 'utf-16le';
  } else if (buffer[0] == 0xFE && buffer[1] == 0xFF) {
    // Big Endian, is NOT supported because Node does not support it (???)
    // See: https://stackoverflow.com/a/14551669/6626414
    throw new ModelCompilerError(ModelCompilerMessages.Error_UTF16BEUnsupported());
  } else {
    // Assume its in UTF-8, with or without a BOM.
    return 'utf-8';
  }
}
